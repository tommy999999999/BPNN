{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3640aa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 499us/step - loss: 1053.7444 - val_loss: 253.0732\n",
      "Epoch 2/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 352us/step - loss: 291.7875 - val_loss: 234.3577\n",
      "Epoch 3/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 351us/step - loss: 307.0810 - val_loss: 231.9677\n",
      "Epoch 4/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 351us/step - loss: 489.5302 - val_loss: 231.6453\n",
      "Epoch 5/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 347us/step - loss: 447.2698 - val_loss: 241.0101\n",
      "Epoch 6/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 349us/step - loss: 350.9335 - val_loss: 235.5103\n",
      "Epoch 7/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 346us/step - loss: 302.8523 - val_loss: 240.3258\n",
      "Epoch 8/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 345us/step - loss: 316.4536 - val_loss: 242.2652\n",
      "Epoch 9/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 348us/step - loss: 280.8061 - val_loss: 236.8358\n",
      "Epoch 10/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 345us/step - loss: 390.3429 - val_loss: 260.6866\n",
      "Epoch 11/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 347us/step - loss: 332.7953 - val_loss: 242.4933\n",
      "Epoch 12/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 346us/step - loss: 249.5802 - val_loss: 254.1891\n",
      "Epoch 13/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 347us/step - loss: 332.8618 - val_loss: 237.2242\n",
      "Epoch 14/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 351us/step - loss: 342.7318 - val_loss: 235.2012\n",
      "Epoch 15/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 363us/step - loss: 281.7760 - val_loss: 238.7655\n",
      "Epoch 16/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 347us/step - loss: 263.9073 - val_loss: 235.4751\n",
      "Epoch 17/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 347us/step - loss: 440.4865 - val_loss: 241.5960\n",
      "Epoch 18/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 347us/step - loss: 275.6813 - val_loss: 243.0950\n",
      "Epoch 19/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 345us/step - loss: 369.5112 - val_loss: 269.2886\n",
      "Epoch 20/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 347us/step - loss: 230.4324 - val_loss: 243.0495\n",
      "Epoch 21/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 346us/step - loss: 313.4017 - val_loss: 246.5705\n",
      "Epoch 22/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 347us/step - loss: 285.4074 - val_loss: 239.5168\n",
      "Epoch 23/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 348us/step - loss: 254.1587 - val_loss: 254.1795\n",
      "Epoch 24/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 349us/step - loss: 238.7106 - val_loss: 228.5336\n",
      "Epoch 25/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 355us/step - loss: 240.1034 - val_loss: 232.7861\n",
      "Epoch 26/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 348us/step - loss: 332.2395 - val_loss: 227.6667\n",
      "Epoch 27/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 349us/step - loss: 195.9540 - val_loss: 238.9271\n",
      "Epoch 28/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 349us/step - loss: 207.6863 - val_loss: 248.7501\n",
      "Epoch 29/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 347us/step - loss: 215.5447 - val_loss: 253.1006\n",
      "Epoch 30/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 350us/step - loss: 290.3593 - val_loss: 222.5806\n",
      "Epoch 31/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 350us/step - loss: 201.4294 - val_loss: 232.1933\n",
      "Epoch 32/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 364us/step - loss: 208.4479 - val_loss: 237.3103\n",
      "Epoch 33/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 351us/step - loss: 384.1992 - val_loss: 209.4097\n",
      "Epoch 34/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 349us/step - loss: 347.3933 - val_loss: 214.5630\n",
      "Epoch 35/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 348us/step - loss: 229.3677 - val_loss: 213.6884\n",
      "Epoch 36/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 354us/step - loss: 298.5423 - val_loss: 206.7708\n",
      "Epoch 37/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 355us/step - loss: 203.5476 - val_loss: 220.6609\n",
      "Epoch 38/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 352us/step - loss: 189.6366 - val_loss: 214.5172\n",
      "Epoch 39/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 354us/step - loss: 206.1456 - val_loss: 231.8940\n",
      "Epoch 40/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 349us/step - loss: 190.8511 - val_loss: 207.4156\n",
      "Epoch 41/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 366us/step - loss: 300.1188 - val_loss: 207.6578\n",
      "Epoch 42/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 353us/step - loss: 182.8032 - val_loss: 211.5119\n",
      "Epoch 43/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 352us/step - loss: 206.1922 - val_loss: 217.4124\n",
      "Epoch 44/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 354us/step - loss: 272.9224 - val_loss: 204.7743\n",
      "Epoch 45/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 354us/step - loss: 264.2507 - val_loss: 193.2350\n",
      "Epoch 46/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 351us/step - loss: 213.9952 - val_loss: 198.3733\n",
      "Epoch 47/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 350us/step - loss: 214.3355 - val_loss: 188.6000\n",
      "Epoch 48/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 359us/step - loss: 193.2950 - val_loss: 194.6019\n",
      "Epoch 49/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 370us/step - loss: 270.4464 - val_loss: 195.7159\n",
      "Epoch 50/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 356us/step - loss: 231.4049 - val_loss: 193.5193\n",
      "Epoch 51/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 353us/step - loss: 230.8691 - val_loss: 194.6670\n",
      "Epoch 52/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 356us/step - loss: 227.9366 - val_loss: 201.9723\n",
      "Epoch 53/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 363us/step - loss: 187.8978 - val_loss: 192.5880\n",
      "Epoch 54/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 376us/step - loss: 213.6511 - val_loss: 199.9491\n",
      "Epoch 55/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 355us/step - loss: 193.9171 - val_loss: 187.8033\n",
      "Epoch 56/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 356us/step - loss: 345.7155 - val_loss: 193.1041\n",
      "Epoch 57/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 349us/step - loss: 172.3311 - val_loss: 208.7204\n",
      "Epoch 58/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 353us/step - loss: 160.9993 - val_loss: 194.5259\n",
      "Epoch 59/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 370us/step - loss: 202.7780 - val_loss: 193.8380\n",
      "Epoch 60/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 352us/step - loss: 189.2641 - val_loss: 190.0939\n",
      "Epoch 61/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 352us/step - loss: 177.2654 - val_loss: 188.4374\n",
      "Epoch 62/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 352us/step - loss: 196.2723 - val_loss: 186.1677\n",
      "Epoch 63/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 370us/step - loss: 209.9332 - val_loss: 183.3039\n",
      "Epoch 64/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 350us/step - loss: 282.5645 - val_loss: 187.3084\n",
      "Epoch 65/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 347us/step - loss: 184.2450 - val_loss: 202.7260\n",
      "Epoch 66/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 347us/step - loss: 203.9953 - val_loss: 189.7481\n",
      "Epoch 67/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 345us/step - loss: 286.8093 - val_loss: 195.0470\n",
      "Epoch 68/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 347us/step - loss: 175.6455 - val_loss: 203.3766\n",
      "Epoch 69/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 346us/step - loss: 206.6574 - val_loss: 186.4765\n",
      "Epoch 70/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 346us/step - loss: 200.1795 - val_loss: 182.3050\n",
      "Epoch 71/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 345us/step - loss: 211.4502 - val_loss: 187.0403\n",
      "Epoch 72/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 345us/step - loss: 170.9704 - val_loss: 218.6368\n",
      "Epoch 73/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 346us/step - loss: 187.0890 - val_loss: 180.2781\n",
      "Epoch 74/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 344us/step - loss: 237.6098 - val_loss: 179.4718\n",
      "Epoch 75/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 346us/step - loss: 141.9230 - val_loss: 196.9661\n",
      "Epoch 76/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 346us/step - loss: 178.5241 - val_loss: 180.4418\n",
      "Epoch 77/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 347us/step - loss: 208.6953 - val_loss: 186.2404\n",
      "Epoch 78/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 346us/step - loss: 197.2104 - val_loss: 192.6191\n",
      "Epoch 79/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 347us/step - loss: 198.9000 - val_loss: 183.1252\n",
      "Epoch 80/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 347us/step - loss: 167.1586 - val_loss: 181.8576\n",
      "Epoch 81/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 347us/step - loss: 204.3730 - val_loss: 180.5472\n",
      "Epoch 82/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 356us/step - loss: 224.6492 - val_loss: 176.3707\n",
      "Epoch 83/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 347us/step - loss: 200.5400 - val_loss: 174.8035\n",
      "Epoch 84/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 348us/step - loss: 275.5649 - val_loss: 188.0387\n",
      "Epoch 85/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 348us/step - loss: 164.9882 - val_loss: 185.5540\n",
      "Epoch 86/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 347us/step - loss: 182.1446 - val_loss: 187.6433\n",
      "Epoch 87/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 346us/step - loss: 156.8684 - val_loss: 189.3394\n",
      "Epoch 88/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 348us/step - loss: 193.0283 - val_loss: 196.5406\n",
      "Epoch 89/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 345us/step - loss: 212.9933 - val_loss: 180.8132\n",
      "Epoch 90/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 346us/step - loss: 185.8486 - val_loss: 191.4214\n",
      "Epoch 91/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 345us/step - loss: 234.0747 - val_loss: 176.9572\n",
      "Epoch 92/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 348us/step - loss: 204.7832 - val_loss: 183.1567\n",
      "Epoch 93/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 352us/step - loss: 188.0882 - val_loss: 172.1700\n",
      "Epoch 94/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 345us/step - loss: 177.4924 - val_loss: 175.7068\n",
      "Epoch 95/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 347us/step - loss: 213.4723 - val_loss: 178.8255\n",
      "Epoch 96/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 346us/step - loss: 187.9576 - val_loss: 181.4435\n",
      "Epoch 97/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 347us/step - loss: 170.8557 - val_loss: 194.0236\n",
      "Epoch 98/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 349us/step - loss: 166.6516 - val_loss: 182.6440\n",
      "Epoch 99/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 346us/step - loss: 180.6068 - val_loss: 185.3622\n",
      "Epoch 100/100\n",
      "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 358us/step - loss: 182.8478 - val_loss: 169.4304\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 789us/step\n",
      "RMSE on test set: 13.330985962315129\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "                                              Product  Forecasted Quantity\n",
      "59              Tramadol HCl 50mg (Rounox) 100&#039;s           369.214996\n",
      "33             Mefenamic Acid (Gopain) Cap 100&#039;s            51.071896\n",
      "30  Ibuprofen + Paracetamol (Fast Relax) Cap 100&#...            46.935532\n",
      "25                                Flanax 550mg Tablet            38.985516\n",
      "1                         Alaxan 525mg Cap 100&#039;s            32.644985\n",
      "46       Paracetamol (CRISTICOL) 500mg Tab 100&#039;s            31.692961\n",
      "0                                Advil 200mg Soft Gel            23.259157\n",
      "34        Mefenamic Acid (Mecid) 500mg Cap 100&#039;s            21.493109\n",
      "28           Ibuprofen (Rheuxan) 400mg Tab 100&#039;s            20.088820\n",
      "12  Celecoxib (Saphlecox/ Celekop/ Emicox/Celecox)...            18.937489\n",
      "22                      Dolfenal 500mg Tab 100&#039;s            18.783121\n",
      "54           Tramadol + Para HCL (Duotram) 100&#039;s            17.659018\n",
      "50                      RM Mefenamic 500mg 100&#039;s            16.898325\n",
      "48                     Pharex Tramadol + Para HCL Tab            15.311589\n",
      "16             Diclofenac (Voren) 50mg Tab 100&#039;s            14.450194\n",
      "32               Medicol 400mg Softgel Cap 100&#039;s            14.378204\n",
      "47          Paracetamol (Pgesic) 500mg Tab 100&#039;s            14.307011\n",
      "17                     Diclofenac sodium (Boren) 50mg            13.458179\n",
      "31               Medicol 200mg Softgel Cap 100&#039;s            11.095980\n",
      "40  Meloxicam (Rumalox/Melotab/Melox 15) 15mg Tab ...            10.995848\n",
      "10                 Celecoxib (CELEBREX) 400mg Capsule             9.985436\n",
      "38                      Meloxicam (MOBIC) 15mg Tablet             9.399742\n",
      "15  Diclofenac (Diclofam/ Trifocid/ Philflam DR) 5...             9.063734\n",
      "41            Meloxicam (Teracam) 15mg Tab 100&#039;s             7.126014\n",
      "55         Tramadol + Para HCL (Para Plus) 100&#039;s             7.024731\n",
      "49                          Ponstan SF 500mg Cap 100s             6.612958\n",
      "58  Tramadol HCl 50mg (Aptradol/ Saphtram/Opiodex)...             6.349223\n",
      "45          Naproxen 500mg (Saphroxen 550) 100&#039;s             6.238183\n",
      "35      Mefenamic Acid (Megyxan) 500mg Cap 100&#039;s             6.134866\n",
      "11        Celecoxib (Capxibb-400) 400mg Cap 20&#039;s             6.129355\n",
      "21                      Dolfenal 250mg Tab 100&#039;s             5.848219\n",
      "26                                   Gardan 500mg Tab             5.503357\n",
      "23                 Eperisone HCl (MYONAL) 50mg Tablet             5.236712\n",
      "57  Tramadol HCl 50mg (Aptradol/ Saphtram) 100&#039;s             5.223380\n",
      "13             Celecoxib 400mg (Saphlecox) 100&#039;s             5.222835\n",
      "56                    Tramadol + Para HCL(DOLCET) Tab             5.037274\n",
      "43                            Muskelax Tab 100&#039;s             4.878551\n",
      "52                                   Skelan 550mg Tab             4.603135\n",
      "39            Meloxicam (Melofar) 15mg Tab 100&#039;s             4.282640\n",
      "2                                   Arcoxia 120mg Tab             4.240893\n",
      "27            Ibuprofen (Fevral) 200mg Tab 100&#039;s             3.972180\n",
      "9                  Celecoxib (CELEBREX) 200mg Capsule             3.904958\n",
      "14                     Diclofenac (CATAFLAM) 50mg Tab             3.854849\n",
      "42                                    Midol 200mg Tab             3.848024\n",
      "24                                Flanax 275mg Tablet             3.339416\n",
      "7                   Celecoxib (CELCOXX) 200mg Capsule             3.326955\n",
      "44                            Naproxen 500mg (Proxen)             3.286489\n",
      "3                                    Arcoxia 60mg Tab             3.276620\n",
      "8                   Celecoxib (CELCOXX) 400mg Capsule             3.174042\n",
      "4                                    Arcoxia 90mg Tab             3.041385\n",
      "5                               Buscopan Venus Tablet             2.946988\n",
      "6         Celecoxib ( Geocoxib ) 400mg Cap 100&#039;s             2.918406\n",
      "36  Mefenamic Acid Syr (Infamix/ Mefechem) 50mg/5m...             2.734421\n",
      "37     Mefenamic Acid Syr (Mefesaph 50) 50mg/5ml 60ml             2.632542\n",
      "20                        Dolan FP FORTE 200mg/5 60ml             2.388288\n",
      "53                  Tramadol + Para HCL (ALGESIA) Tab             2.276908\n",
      "18                    Difflam Forte Throat Spray 15ml             2.237121\n",
      "19                              Dolan FP 100mg/5 60ml             2.148341\n",
      "51                                  Sarimax 550mg Tab             1.989392\n",
      "29                        Ibuprofen (Sqfen) 200mg/5ml             1.906968\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Load your data\n",
    "df = pd.read_csv('nsaids.csv')\n",
    "\n",
    "# Convert 'Sold_date' to datetime format\n",
    "df['Sold_date'] = pd.to_datetime(df['Sold_date'], format='%m/%d/%y')\n",
    "\n",
    "# Set 'Sold_date' as the index\n",
    "df.set_index('Sold_date', inplace=True)\n",
    "\n",
    "# Aggregate sales data on a weekly basis for each product\n",
    "weekly_sales = df.groupby([pd.Grouper(freq='W'), \n",
    "'Product_details']).agg(total_sold=('Sold_quantity', 'sum')).reset_index()\n",
    "\n",
    "# Apply one-hot encoding to 'Product_details'\n",
    "weekly_sales_encoded = pd.concat([weekly_sales, pd.get_dummies(weekly_sales['Product_details'], \n",
    "prefix='product')], axis=1).drop('Product_details', axis=1)\n",
    "\n",
    "# Extract year and week number from 'Sold_date' for temporal features\n",
    "weekly_sales_encoded['year'] = weekly_sales_encoded['Sold_date'].dt.year\n",
    "weekly_sales_encoded['week_of_year'] = weekly_sales_encoded['Sold_date'].dt.isocalendar().week\n",
    "\n",
    "# Normalize the temporal features \n",
    "scaler = MinMaxScaler()\n",
    "features_columns = ['year', 'week_of_year'] + [col for col in weekly_sales_encoded.columns \n",
    "if col.startswith('product')]\n",
    "weekly_sales_encoded[features_columns] = scaler.fit_transform(weekly_sales_encoded[features_columns])\n",
    "\n",
    "# Split the data into features and target\n",
    "X = weekly_sales_encoded[features_columns].values\n",
    "y = weekly_sales_encoded['total_sold'].values\n",
    "\n",
    "# Adjusting the dataset split into 70% training, 15% validation, and 15% testing\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.15, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.1765, random_state=42)  # 0.1765 of 85% is roughly 15% of the whole\n",
    "\n",
    "# Define the model architecture\n",
    "model = Sequential([\n",
    "    Input(shape=(X_train.shape[1],)),\n",
    "    Dense(100, activation='relu'),  # Additional hidden layer\n",
    "    Dense(100, activation='relu'),  # Additional hidden layer\n",
    "    Dense(100, activation='relu'),  # Additional hidden layer\n",
    "    Dense(1, activation='relu')    # Output layer\n",
    "])\n",
    "\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(), loss='mean_squared_error')\n",
    "\n",
    "# Train the model with the validation data\n",
    "model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=100, batch_size=10)\n",
    "\n",
    "# Evaluate the model\n",
    "predictions = model.predict(X_test)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, predictions))\n",
    "print(f'RMSE on test set: {rmse}')\n",
    "\n",
    "# Prepare forecasting features for Feb 1 to 8, 2024, correctly this time\n",
    "forecast_results = []\n",
    "for product_col in [col for col in features_columns if col.startswith('product')]:\n",
    "    forecast_period_features = np.zeros((1, len(features_columns)))\n",
    "    # Find the index of the year, week_of_year, and current product column\n",
    "    year_index = features_columns.index('year')\n",
    "    week_of_year_index = features_columns.index('week_of_year')\n",
    "    product_index = features_columns.index(product_col)\n",
    "    \n",
    "    # Normalize year and week_of_year values\n",
    "    forecast_period_features[0, year_index] = (2024 - scaler.data_min_[year_index]) / (scaler.data_max_[year_index] - scaler.data_min_[year_index])\n",
    "    forecast_period_features[0, week_of_year_index] = (5 - scaler.data_min_[week_of_year_index]) / (scaler.data_max_[week_of_year_index] - scaler.data_min_[week_of_year_index])\n",
    "    forecast_period_features[0, product_index] = 1  # Activate current product\n",
    "\n",
    "    # Predict and store the result\n",
    "    predicted_quantity = model.predict(forecast_period_features).flatten()[0]\n",
    "    product_name = product_col.replace('product_', '')  # Assuming prefix 'product_' is used\n",
    "    forecast_results.append((product_name, predicted_quantity))\n",
    "\n",
    "# Display forecasted quantities for each product in a DataFrame\n",
    "forecast_df = pd.DataFrame(forecast_results, columns=['Product', 'Forecasted Quantity']).sort_values(by='Forecasted Quantity', ascending=False)\n",
    "print(forecast_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e95d639",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
